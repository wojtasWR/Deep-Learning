{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c367c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform_train)\n",
    "\n",
    "trainset, validationset = random_split(dataset, [40000, 10000])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, prefetch_factor=2, num_workers=8,pin_memory=True)\n",
    "validationloader = torch.utils.data.DataLoader(validationset, batch_size=batch_size, shuffle=True, prefetch_factor=2, num_workers=8,pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, prefetch_factor=2, num_workers=8,pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf7f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(net, dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    \n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "def numel(m: torch.nn.Module, only_trainable: bool = False):\n",
    "    \"\"\"\n",
    "    returns the total number of parameters used by `m` (only counting\n",
    "    shared parameters once); if `only_trainable` is True, then only\n",
    "    includes parameters with `requires_grad = True`\n",
    "    \"\"\"\n",
    "    parameters = m.parameters()\n",
    "    if only_trainable:\n",
    "        parameters = list(p for p in parameters if p.requires_grad)\n",
    "    unique = dict((p.data_ptr(), p) for p in parameters).values()\n",
    "    return sum(p.numel() for p in unique)\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearBlock, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        self.bn = nn.BatchNorm1d(output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.bn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ResLinearBlock(nn.Module):\n",
    "    def __init__(self, input_dim, dim):\n",
    "        super(ResLinearBlock, self).__init__()\n",
    "        self.fc1 = LinearBlock(input_dim, dim)\n",
    "        self.fc2 = LinearBlock(dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        out += x    \n",
    "        return out\n",
    "    \n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_dim ,output_dim, layers, block = LinearBlock, num_classes=10):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers = layers\n",
    "        self.block = block\n",
    "        self.feed = self.make_layers()\n",
    "        \n",
    "\n",
    "    def make_layers(self):\n",
    "        network_layers = []\n",
    "        network_layers.append(\n",
    "            LinearBlock(self.input_dim, self.layers[0])\n",
    "        )\n",
    "        \n",
    "        for i in range(len(self.layers) - 1):\n",
    "            network_layers.append(\n",
    "                self.block(self.layers[0], self.layers[i+1])\n",
    "            )\n",
    "            \n",
    "        network_layers.append(\n",
    "            nn.Linear(self.layers[0], self.output_dim)\n",
    "        )\n",
    "        \n",
    "\n",
    "        return nn.Sequential(*network_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.feed(x)\n",
    "        return x\n",
    "\n",
    "class Net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3,padding=1,stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64,64,3,padding=1,stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64*16*16, 600)\n",
    "        self.bn3 = nn.BatchNorm1d(600)\n",
    "        self.fc3 = nn.Linear(600, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu((self.bn3(self.fc1(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 76, 3,stride=1,padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(76)\n",
    "        self.conv2 = nn.Conv2d(76, 76, 3,stride=1,padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(76)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv3 = nn.Conv2d(76, 126, 3,stride=1,padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(126)\n",
    "        self.conv4 = nn.Conv2d(126, 126, 3,stride=1,padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(126)\n",
    "        self.conv5 = nn.Conv2d(126, 148, 3,stride=1,padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(148)\n",
    "        self.conv6 = nn.Conv2d(148, 148, 3,stride=1,padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(148)\n",
    "        self.conv7 = nn.Conv2d(148, 148, 3,stride=1,padding=1)\n",
    "        self.bn7 = nn.BatchNorm2d(148)\n",
    "        self.conv8 = nn.Conv2d(148, 148, 3,stride=1,padding=1)\n",
    "        self.bn8 = nn.BatchNorm2d(148)\n",
    "        self.fc1 = nn.Linear(148 *4*4, 1200)\n",
    "        self.bn9 = nn.BatchNorm1d(1200)\n",
    "        self.fc2 = nn.Linear(1200, 1200)\n",
    "        self.bn10 = nn.BatchNorm1d(1200)\n",
    "        self.fc3 = nn.Linear(1200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (F.relu(self.bn1(self.conv1(x))))\n",
    "        x = (F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        torch.nn.Dropout(p=0.2, inplace=False)\n",
    "        \n",
    "        \n",
    "        x = (F.relu(self.bn3(self.conv3(x))))\n",
    "        x = (F.relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(self.conv4(x))\n",
    "        torch.nn.Dropout(p=0.3, inplace=False)\n",
    "        \n",
    "        \n",
    "        x = (F.relu(self.bn5(self.conv5(x))))\n",
    "        x = (F.relu(self.bn6(self.conv6(x))))\n",
    "        x = (F.relu(self.bn7(self.conv7(x))))\n",
    "        x = (F.relu(self.bn8(self.conv8(x))))\n",
    "        x = self.pool(self.conv8(x))\n",
    "        torch.nn.Dropout(p=0.37, inplace=False)\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.bn9(self.fc1(x)))\n",
    "        x = F.relu(self.bn10(self.fc2(x)))\n",
    "        torch.nn.Dropout(p=0.42, inplace=False)\n",
    "        x =self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 32\n",
    "        self.conv = conv3x3(3, 32)\n",
    "        self.bn = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 128, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 256*2, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 256*2, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(256*2, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c5ffa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.171146\n",
      "LinearNet(\n",
      "  (feed): Sequential(\n",
      "    (0): LinearBlock(\n",
      "      (fc): Linear(in_features=3072, out_features=256, bias=True)\n",
      "      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (19): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (20): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (21): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (22): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (23): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (24): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (25): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (26): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (27): ResLinearBlock(\n",
      "      (fc1): LinearBlock(\n",
      "        (fc): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (fc2): LinearBlock(\n",
      "        (fc): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (28): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "\n",
    "#teacher = ResNet(ResidualBlock, [4, 6, 8]).to(device)\n",
    "\n",
    "teacher = Net().to(device)\n",
    "teacher.load_state_dict(torch.load(\"./Teacher260\"))\n",
    "layers = [256 for i in range(8)] + [512 for i in range(8)] + [1024 for i in range(12)]\n",
    "student = LinearNet(32*32*3, 10, layers, block = ResLinearBlock).to(device)\n",
    "\n",
    "#student = LinearNet(32*32*3, 10, layers).to(device)\n",
    "#student.load_state_dict(torch.load(\"./distillation2_linear_3_490\"))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(student.parameters())#, #momentum=0.9)\n",
    "epochs_done=0\n",
    "print(numel(student)/10**6)\n",
    "\n",
    "train_accuracy = []\n",
    "validation_accuracy = []\n",
    "\n",
    "validation_loss = []\n",
    "train_loss = []\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244c5be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, training loss: 353.6999883842621, validation loss: 0.11232032275830324, validation accuracy: 21.46%\n",
      "Epoch: 30, training loss: 295.3253703182272, validation loss: 0.11232032275830324, validation accuracy: 21.46%\n",
      "Epoch: 31, training loss: 283.1195007764024, validation loss: 0.11232032275830324, validation accuracy: 21.46%\n",
      "Epoch: 32, training loss: 273.5660135163795, validation loss: 0.11232032275830324, validation accuracy: 21.46%\n",
      "Epoch: 33, training loss: 270.58292212383185, validation loss: 0.11232032275830324, validation accuracy: 21.46%\n",
      "Epoch: 34, training loss: 266.1067443676621, validation loss: 0.10897185395543392, validation accuracy: 31.51%\n",
      "Epoch: 35, training loss: 264.1522991953133, validation loss: 0.10897185395543392, validation accuracy: 31.51%\n",
      "Epoch: 36, training loss: 258.86784139225443, validation loss: 0.10897185395543392, validation accuracy: 31.51%\n",
      "Epoch: 37, training loss: 259.354654982531, validation loss: 0.10897185395543392, validation accuracy: 31.51%\n",
      "Epoch: 38, training loss: 259.75923091989216, validation loss: 0.10897185395543392, validation accuracy: 31.51%\n",
      "Epoch: 39, training loss: 254.91186513664056, validation loss: 0.10508123458100435, validation accuracy: 36.31%\n",
      "Epoch: 40, training loss: 251.2313295216251, validation loss: 0.10508123458100435, validation accuracy: 36.31%\n",
      "Epoch: 41, training loss: 249.5999489777369, validation loss: 0.10508123458100435, validation accuracy: 36.31%\n",
      "Epoch: 42, training loss: 249.85411813337387, validation loss: 0.10508123458100435, validation accuracy: 36.31%\n",
      "Epoch: 43, training loss: 249.70335004575927, validation loss: 0.10508123458100435, validation accuracy: 36.31%\n",
      "Epoch: 44, training loss: 248.00152167021704, validation loss: 0.10205179246333547, validation accuracy: 38.4%\n",
      "Epoch: 45, training loss: 246.06544391396906, validation loss: 0.10205179246333547, validation accuracy: 38.4%\n",
      "Epoch: 46, training loss: 244.5617381722761, validation loss: 0.10205179246333547, validation accuracy: 38.4%\n",
      "Epoch: 47, training loss: 242.4981939164613, validation loss: 0.10205179246333547, validation accuracy: 38.4%\n",
      "Epoch: 48, training loss: 240.52769269859246, validation loss: 0.10205179246333547, validation accuracy: 38.4%\n",
      "Epoch: 49, training loss: 240.40170128045796, validation loss: 0.10100239666942985, validation accuracy: 38.8%\n",
      "Epoch: 50, training loss: 241.83083456511304, validation loss: 0.10100239666942985, validation accuracy: 38.8%\n",
      "Epoch: 51, training loss: 242.55165407350293, validation loss: 0.10100239666942985, validation accuracy: 38.8%\n",
      "Epoch: 52, training loss: 245.37355511850888, validation loss: 0.10100239666942985, validation accuracy: 38.8%\n",
      "Epoch: 53, training loss: 239.9513624571532, validation loss: 0.10100239666942985, validation accuracy: 38.8%\n",
      "Epoch: 54, training loss: 238.77821373920426, validation loss: 0.10144804858674224, validation accuracy: 40.01%\n",
      "Epoch: 55, training loss: 239.04446157023276, validation loss: 0.10144804858674224, validation accuracy: 40.01%\n",
      "Epoch: 56, training loss: 245.19705437201324, validation loss: 0.10144804858674224, validation accuracy: 40.01%\n",
      "Epoch: 57, training loss: 246.57522528032192, validation loss: 0.10144804858674224, validation accuracy: 40.01%\n",
      "Epoch: 58, training loss: 240.61637340878562, validation loss: 0.10144804858674224, validation accuracy: 40.01%\n",
      "Epoch: 59, training loss: 239.4787397483905, validation loss: 0.1003306570630043, validation accuracy: 40.91%\n",
      "Epoch: 60, training loss: 236.9708262031989, validation loss: 0.1003306570630043, validation accuracy: 40.91%\n",
      "Epoch: 61, training loss: 237.8377674918255, validation loss: 0.1003306570630043, validation accuracy: 40.91%\n",
      "Epoch: 62, training loss: 236.4542603382022, validation loss: 0.1003306570630043, validation accuracy: 40.91%\n",
      "Epoch: 63, training loss: 238.97527925905177, validation loss: 0.1003306570630043, validation accuracy: 40.91%\n",
      "Epoch: 64, training loss: 237.34058493132397, validation loss: 0.09941491959855342, validation accuracy: 41.37%\n",
      "Epoch: 65, training loss: 237.01377949894095, validation loss: 0.09941491959855342, validation accuracy: 41.37%\n",
      "Epoch: 72, training loss: 233.9195397883248, validation loss: 0.09915828198576584, validation accuracy: 41.87%\n",
      "Epoch: 73, training loss: 231.66938660181074, validation loss: 0.09915828198576584, validation accuracy: 41.87%\n",
      "Epoch: 74, training loss: 233.33290993756918, validation loss: 0.1004253196864365, validation accuracy: 40.68%\n",
      "Epoch: 75, training loss: 234.5299889386607, validation loss: 0.1004253196864365, validation accuracy: 40.68%\n",
      "Epoch: 76, training loss: 233.01826274251823, validation loss: 0.1004253196864365, validation accuracy: 40.68%\n",
      "Epoch: 77, training loss: 234.90295835301055, validation loss: 0.1004253196864365, validation accuracy: 40.68%\n",
      "Epoch: 84, training loss: 228.61748774019216, validation loss: 0.09592313366010785, validation accuracy: 42.52%\n",
      "Epoch: 85, training loss: 229.76338851232353, validation loss: 0.09592313366010785, validation accuracy: 42.52%\n",
      "Epoch: 86, training loss: 226.7611911611809, validation loss: 0.09592313366010785, validation accuracy: 42.52%\n",
      "Epoch: 87, training loss: 228.3733063843653, validation loss: 0.09592313366010785, validation accuracy: 42.52%\n",
      "Epoch: 88, training loss: 228.45237031910685, validation loss: 0.09592313366010785, validation accuracy: 42.52%\n",
      "Epoch: 89, training loss: 227.8606898885808, validation loss: 0.09523234866225185, validation accuracy: 44.13%\n",
      "Epoch: 90, training loss: 225.5911588829169, validation loss: 0.09523234866225185, validation accuracy: 44.13%\n",
      "Epoch: 91, training loss: 226.3452681823193, validation loss: 0.09523234866225185, validation accuracy: 44.13%\n",
      "Epoch: 92, training loss: 226.8247390759287, validation loss: 0.09523234866225185, validation accuracy: 44.13%\n",
      "Epoch: 93, training loss: 226.08431556035845, validation loss: 0.09523234866225185, validation accuracy: 44.13%\n",
      "Epoch: 94, training loss: 224.6574581564666, validation loss: 0.0943896374784601, validation accuracy: 44.66%\n",
      "Epoch: 95, training loss: 225.07432773611467, validation loss: 0.0943896374784601, validation accuracy: 44.66%\n",
      "Epoch: 96, training loss: 224.0834890751957, validation loss: 0.0943896374784601, validation accuracy: 44.66%\n",
      "Epoch: 97, training loss: 224.351621330023, validation loss: 0.0943896374784601, validation accuracy: 44.66%\n",
      "Epoch: 98, training loss: 224.9699782592187, validation loss: 0.0943896374784601, validation accuracy: 44.66%\n",
      "Epoch: 99, training loss: 224.20085111057597, validation loss: 0.09498663217975543, validation accuracy: 44.21%\n",
      "Epoch: 100, training loss: 221.83485561031833, validation loss: 0.09498663217975543, validation accuracy: 44.21%\n",
      "Epoch: 101, training loss: 219.73133209937092, validation loss: 0.09498663217975543, validation accuracy: 44.21%\n",
      "Epoch: 102, training loss: 222.55392685730615, validation loss: 0.09498663217975543, validation accuracy: 44.21%\n",
      "Epoch: 103, training loss: 226.11437380495215, validation loss: 0.09498663217975543, validation accuracy: 44.21%\n",
      "Epoch: 104, training loss: 222.91515576352683, validation loss: 0.09629673273183215, validation accuracy: 43.86%\n",
      "Epoch: 105, training loss: 226.32229490868085, validation loss: 0.09629673273183215, validation accuracy: 43.86%\n",
      "Epoch: 106, training loss: 227.26201632082032, validation loss: 0.09629673273183215, validation accuracy: 43.86%\n",
      "Epoch: 107, training loss: 222.97192113431956, validation loss: 0.09629673273183215, validation accuracy: 43.86%\n",
      "Epoch: 108, training loss: 221.82095812147764, validation loss: 0.09629673273183215, validation accuracy: 43.86%\n",
      "Epoch: 109, training loss: 222.0037552930528, validation loss: 0.09561032313519181, validation accuracy: 44.99%\n",
      "Epoch: 110, training loss: 222.6323820286698, validation loss: 0.09561032313519181, validation accuracy: 44.99%\n",
      "Epoch: 111, training loss: 221.00979456050192, validation loss: 0.09561032313519181, validation accuracy: 44.99%\n",
      "Epoch: 112, training loss: 223.66591029170993, validation loss: 0.09561032313519181, validation accuracy: 44.99%\n",
      "Epoch: 113, training loss: 221.2811323277372, validation loss: 0.09561032313519181, validation accuracy: 44.99%\n",
      "Epoch: 114, training loss: 219.88194618194555, validation loss: 0.09465156917054302, validation accuracy: 44.05%\n",
      "Epoch: 115, training loss: 222.12184785670334, validation loss: 0.09465156917054302, validation accuracy: 44.05%\n",
      "Epoch: 116, training loss: 222.61200627372014, validation loss: 0.09465156917054302, validation accuracy: 44.05%\n",
      "Epoch: 117, training loss: 221.41068243388656, validation loss: 0.09465156917054302, validation accuracy: 44.05%\n",
      "Epoch: 118, training loss: 222.06666745139276, validation loss: 0.09465156917054302, validation accuracy: 44.05%\n",
      "Epoch: 119, training loss: 223.01898928200177, validation loss: 0.09514426005383332, validation accuracy: 44.14%\n",
      "Epoch: 120, training loss: 221.05454947454058, validation loss: 0.09514426005383332, validation accuracy: 44.14%\n",
      "Epoch: 121, training loss: 223.07106895232982, validation loss: 0.09514426005383332, validation accuracy: 44.14%\n",
      "Epoch: 122, training loss: 220.64251529397345, validation loss: 0.09514426005383332, validation accuracy: 44.14%\n",
      "Epoch: 123, training loss: 221.20878831971828, validation loss: 0.09514426005383332, validation accuracy: 44.14%\n",
      "Epoch: 124, training loss: 222.1607097995291, validation loss: 0.09537430690267147, validation accuracy: 44.6%\n",
      "Epoch: 125, training loss: 221.10420916078567, validation loss: 0.09537430690267147, validation accuracy: 44.6%\n",
      "Epoch: 126, training loss: 219.62069277614475, validation loss: 0.09537430690267147, validation accuracy: 44.6%\n",
      "Epoch: 127, training loss: 220.17028902663145, validation loss: 0.09537430690267147, validation accuracy: 44.6%\n",
      "Epoch: 128, training loss: 223.40315362030836, validation loss: 0.09537430690267147, validation accuracy: 44.6%\n",
      "Epoch: 129, training loss: 223.3822261116999, validation loss: 0.09425362274767114, validation accuracy: 43.88%\n",
      "Epoch: 130, training loss: 223.54963728673368, validation loss: 0.09425362274767114, validation accuracy: 43.88%\n",
      "Epoch: 131, training loss: 219.96993338078667, validation loss: 0.09425362274767114, validation accuracy: 43.88%\n",
      "Epoch: 132, training loss: 218.71141834121593, validation loss: 0.09425362274767114, validation accuracy: 43.88%\n",
      "Epoch: 133, training loss: 219.10380487617633, validation loss: 0.09425362274767114, validation accuracy: 43.88%\n",
      "Epoch: 134, training loss: 219.82184832338527, validation loss: 0.09337880623598512, validation accuracy: 45.17%\n",
      "Epoch: 135, training loss: 220.9265420331871, validation loss: 0.09337880623598512, validation accuracy: 45.17%\n",
      "Epoch: 136, training loss: 220.70980844734382, validation loss: 0.09337880623598512, validation accuracy: 45.17%\n",
      "Epoch: 137, training loss: 221.81812372505425, validation loss: 0.09337880623598512, validation accuracy: 45.17%\n",
      "Epoch: 138, training loss: 221.6775878161025, validation loss: 0.09337880623598512, validation accuracy: 45.17%\n",
      "Epoch: 139, training loss: 218.85916220158745, validation loss: 0.09397298903562702, validation accuracy: 44.76%\n",
      "Epoch: 140, training loss: 222.4215020331695, validation loss: 0.09397298903562702, validation accuracy: 44.76%\n",
      "Epoch: 141, training loss: 225.28930491805554, validation loss: 0.09397298903562702, validation accuracy: 44.76%\n",
      "Epoch: 142, training loss: 221.41587283498674, validation loss: 0.09397298903562702, validation accuracy: 44.76%\n",
      "Epoch: 143, training loss: 219.18005363152827, validation loss: 0.09397298903562702, validation accuracy: 44.76%\n",
      "Epoch: 144, training loss: 221.57675471431833, validation loss: 0.09373076343670106, validation accuracy: 44.81%\n",
      "Epoch: 145, training loss: 222.0407079447738, validation loss: 0.09373076343670106, validation accuracy: 44.81%\n",
      "Epoch: 146, training loss: 220.84966220046348, validation loss: 0.09373076343670106, validation accuracy: 44.81%\n",
      "Epoch: 147, training loss: 220.39288724070076, validation loss: 0.09373076343670106, validation accuracy: 44.81%\n",
      "Epoch: 148, training loss: 221.20423477453838, validation loss: 0.09373076343670106, validation accuracy: 44.81%\n",
      "Epoch: 149, training loss: 217.9259299040795, validation loss: 0.09304126150285204, validation accuracy: 45.27%\n",
      "Epoch: 150, training loss: 220.3912262378262, validation loss: 0.09304126150285204, validation accuracy: 45.27%\n",
      "Epoch: 151, training loss: 218.2369061447889, validation loss: 0.09304126150285204, validation accuracy: 45.27%\n",
      "Epoch: 152, training loss: 218.1437196876642, validation loss: 0.09304126150285204, validation accuracy: 45.27%\n",
      "Epoch: 153, training loss: 219.6629235761656, validation loss: 0.09304126150285204, validation accuracy: 45.27%\n",
      "Epoch: 154, training loss: 221.80720968987103, validation loss: 0.09267373922734688, validation accuracy: 45.84%\n",
      "Epoch: 155, training loss: 219.63445586791698, validation loss: 0.09267373922734688, validation accuracy: 45.84%\n",
      "Epoch: 156, training loss: 217.85727317816168, validation loss: 0.09267373922734688, validation accuracy: 45.84%\n",
      "Epoch: 157, training loss: 217.9390781118547, validation loss: 0.09267373922734688, validation accuracy: 45.84%\n",
      "Epoch: 158, training loss: 219.41740719235545, validation loss: 0.09267373922734688, validation accuracy: 45.84%\n",
      "Epoch: 159, training loss: 218.16752975744853, validation loss: 0.09160002793830174, validation accuracy: 45.7%\n",
      "Epoch: 160, training loss: 216.55119779417285, validation loss: 0.09160002793830174, validation accuracy: 45.7%\n",
      "Epoch: 161, training loss: 219.01401786712574, validation loss: 0.09160002793830174, validation accuracy: 45.7%\n",
      "Epoch: 162, training loss: 217.24948973460997, validation loss: 0.09160002793830174, validation accuracy: 45.7%\n",
      "Epoch: 163, training loss: 215.01153483211374, validation loss: 0.09160002793830174, validation accuracy: 45.7%\n",
      "Epoch: 164, training loss: 215.87798498075804, validation loss: 0.09266462276140466, validation accuracy: 45.94%\n",
      "Epoch: 165, training loss: 214.2037272273874, validation loss: 0.09266462276140466, validation accuracy: 45.94%\n",
      "Epoch: 166, training loss: 215.2310642276028, validation loss: 0.09266462276140466, validation accuracy: 45.94%\n",
      "Epoch: 167, training loss: 216.6684887582918, validation loss: 0.09266462276140466, validation accuracy: 45.94%\n",
      "Epoch: 168, training loss: 216.6461824855392, validation loss: 0.09266462276140466, validation accuracy: 45.94%\n",
      "Epoch: 169, training loss: 217.07987211913084, validation loss: 0.09260304921712631, validation accuracy: 45.58%\n",
      "Epoch: 170, training loss: 214.5969467102002, validation loss: 0.09260304921712631, validation accuracy: 45.58%\n",
      "Epoch: 171, training loss: 214.92598624187437, validation loss: 0.09260304921712631, validation accuracy: 45.58%\n",
      "Epoch: 172, training loss: 214.84873448074867, validation loss: 0.09260304921712631, validation accuracy: 45.58%\n",
      "Epoch: 173, training loss: 212.8289417789114, validation loss: 0.09260304921712631, validation accuracy: 45.58%\n",
      "Epoch: 174, training loss: 211.96452605199394, validation loss: 0.09154320403169362, validation accuracy: 46.37%\n",
      "Epoch: 175, training loss: 214.43581381829097, validation loss: 0.09154320403169362, validation accuracy: 46.37%\n",
      "Epoch: 176, training loss: 214.7554017640382, validation loss: 0.09154320403169362, validation accuracy: 46.37%\n",
      "Epoch: 177, training loss: 212.8577084262625, validation loss: 0.09154320403169362, validation accuracy: 46.37%\n"
     ]
    }
   ],
   "source": [
    "#Training Loop\n",
    "for epoch in range(300):  # loop over the dataset multiple times\n",
    "    epochs_done += 1 \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        inputs, _ = data\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = student(inputs)\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            \n",
    "        loss = criterion(outputs, teacher_outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    #if epochs_done % 10 == 0 and epochs_done!=0 :\n",
    "        #torch.save(student.state_dict(), f\"./distillation2_linear_3_{epochs_done}\")\n",
    "        \n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "        with torch.no_grad(): \n",
    "            v_loss = 0.0\n",
    "            for j, data in enumerate(validationloader, 0):\n",
    "                inputs, _ = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                teacher_outputs = F.softmax(teacher(inputs))\n",
    "                \n",
    "                outputs = F.softmax(student(inputs))\n",
    "                v_loss += criterion(outputs, teacher_outputs).item()\n",
    "                \n",
    "            validation_loss.append(v_loss/j)\n",
    "            \n",
    "            v_accuracy = compute_accuracy(student, validationloader)\n",
    "            validation_accuracy.append(v_accuracy)\n",
    "            \n",
    "            train_loss.append(running_loss/i)\n",
    "            t_accuracy = compute_accuracy(student, trainloader)\n",
    "            train_accuracy.append(t_accuracy)\n",
    "    \n",
    "    print(f\"Epoch: {epochs_done}, training loss: {running_loss/i}, validation loss: {v_loss/j}, validation accuracy: {v_accuracy}%\")\n",
    "                \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733b724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs_plot = [5*(i + 1) for i in range(epoch//5 + 1)]\n",
    "\n",
    "plt.plot(epochs_plot, validation_loss)\n",
    "plt.plot(epochs_plot, train_loss)\n",
    "plt.title(\"Loss Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Validation Loss\", \"Training Loss\"])\n",
    "#plt.show()\n",
    "plt.savefig(\"linear_deep_distillation_300_loss.png\", dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "        frameon=None, metadata=None)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs_plot, validation_accuracy)\n",
    "plt.plot(epochs_plot, train_accuracy)\n",
    "plt.title(\"Accuracy Plot\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Validation Accuracy\", \"Training Accuracy\"])\n",
    "#plt.show()\n",
    "plt.savefig(\"linear_deep_distillation_300_acc.png\", dpi=None, facecolor='w', edgecolor='w',\n",
    "        orientation='portrait', papertype=None, format=None,\n",
    "        transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "        frameon=None, metadata=None)\n",
    "\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "64a3eee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 59.99%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = student(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "43f7b650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.2997e-12, 2.6171e-12, 1.2200e-17, 1.6800e-12, 4.8749e-20, 7.8765e-22,\n",
       "        7.7968e-17, 1.9507e-17, 1.0000e+00, 2.6583e-13], device='cuda:0',\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973c089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
